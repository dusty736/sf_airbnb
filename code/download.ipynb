{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sublime-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wget\n",
    "import urllib\n",
    "import gzip\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "favorite-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dir():\n",
    "    \"\"\"Create needed directory structure for data to be stored locally.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir = sys.path[0] + \"/../\"\n",
    "    \n",
    "    data_dir = root_dir + \"data/\"\n",
    "    raw_dir = data_dir + \"raw/\"\n",
    "    processed_dir = data_dir + \"processed/\"\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except:\n",
    "        print(\"Data directory exists\")\n",
    "    try:\n",
    "        os.mkdir(raw_dir)\n",
    "    except:\n",
    "        print(\"Raw data directory exists\")\n",
    "    try:\n",
    "        os.mkdir(processed_dir)\n",
    "    except:\n",
    "        print(\"processed data directory exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confirmed-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(date, url_list, city, print_option):\n",
    "    \"\"\"Retreives the date with data most close to the provided date\n",
    "    Parameters\n",
    "    ----------\n",
    "    date: datetime.datetime.date\n",
    "        Datetime object that is the desired date for data\n",
    "    url_list: list\n",
    "        List of URLs to be parsed\n",
    "    city: string\n",
    "        City of desired data, 'san-francisco' default\n",
    "    print_option: bool\n",
    "        True or False indicating whether you want the dates available for city fo be printed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    closest_date: string\n",
    "        string of closest date to provided date data is avaiable\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(date, datetime.date) == False:\n",
    "        print(\"date object must be datetime.date\")\n",
    "        return\n",
    "        \n",
    "    if len(url_list) == 0:\n",
    "        print(\"Please provide non-empty list of urls\")\n",
    "        return\n",
    "                \n",
    "    # Get list of unique dates data is available\n",
    "    date_list = set([datetime.datetime.strptime(url.split(\"/\")[-3], \"%Y-%m-%d\") for url in url_list])\n",
    "    \n",
    "    # Print dates data is available\n",
    "    if print_option==True:\n",
    "        print(\"hi\")\n",
    "        print(f\"Data for {city} is available for:\")\n",
    "        print(\"\")\n",
    "        for d in date_list:\n",
    "            print(d)\n",
    "        \n",
    "    # Find data closest to desired date\n",
    "    min_dist = 10_000\n",
    "    closest_date = ''\n",
    "    if len(date_list) > 0:\n",
    "        for data_date in date_list:\n",
    "            delta = abs((date - data_date).days)\n",
    "            if delta < min_dist:\n",
    "                min_dist = delta\n",
    "                closest_date = data_date\n",
    "                \n",
    "    return(closest_date.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "settled-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_urls, clear_dir=False):\n",
    "    \"\"\"Download the data to local machine\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_urls: list\n",
    "        List of urls for city at closest to desired date\n",
    "    clear_dir: bool\n",
    "        Boolean indicating whether the raw data should be cleared out.  False by default.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    Some code borrowed from the following:\n",
    "    https://stackabuse.com/download-files-with-python/\n",
    "    https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n",
    "    \"\"\"\n",
    "    \n",
    "    if clear_dir == True:\n",
    "        raw_dir = sys.path[0] + \"/../data/raw/\"\n",
    "        files = glob.glob(raw_dir + \"*\")\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "            \n",
    "    for file in data_urls:\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        file_path = sys.path[0] + \"/../data/raw/\" + file_name\n",
    "        if os.path.isfile(file_path):\n",
    "            next\n",
    "        else:\n",
    "            wget.download(file, file_path)\n",
    "    \n",
    "        if '.gz' in file_name:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(file_path[:-3], 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "                \n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "standing-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download airbnb content from html\n",
    "url = \"http://insideairbnb.com/get-the-data.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unexpected-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all urls associated with city\n",
    "data_links = []\n",
    "city = \"san-francisco\"\n",
    "for link in soup.find_all('a', href=True):\n",
    "    url = link['href']\n",
    "    if (city in url) and (\".csv\" in url):\n",
    "        data_links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "marine-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links closest to selected date\n",
    "date = datetime.datetime.strptime(\"2021-08-04\", \"%Y-%m-%d\")\n",
    "closest_date = get_date(date, data_links, city, print_option=False)\n",
    "final_data_links = [link for link in data_links if closest_date in link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forbidden-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "download_data(final_data_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-richardson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
